{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch # library\nimport torch.nn as nn # importing nural network \n\nclass NeuralNetwork(nn.Module): # create class\n    def __init__(self):\n        super(NeuralNetwork,self).__init__()\n        self.fc1 = nn.Linear(1,8) # first layer \n        self.fc2 = nn.Linear(8,16) # second layer\n        self.fc3 = nn.Linear(16,32) # third layer\n        self.fc4 = nn.Linear(32,64) # fourth layer\n        self.fc5 = nn.Linear(64,6) # fifth layer\n\n    def forward(self,x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))\n        x = torch.relu(self.fc4(x))\n        x = torch.sigmoid(self.fc5(x))\n        return x\n    \n\nmodel = NeuralNetwork()\n\nmodel\n\ninput_tensor = torch.tensor([[1.0]])\n\noutput = model(input_tensor)\n\noutput\n\n# criterion = nn.BCELoss()\n\ncriterion = nn.BCEWithLogitsLoss()  # Remove sigmoid from forward()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n\ndata_inputs = torch.randn(100,1) # Dummy data for training\n\ntargets = torch.randint(0,2,(100,6)).float() \n# Generates numbers in between 0 and 1 only\n# (100,6) creates 100 samples with 6 lables each\n# float() for pytorch loss function(must**)\n\nepoches = 20 # model run 20 iteration through the entire model\n\nfor epoch in range(epoches):\n    optimizer.zero_grad() # for clearing the accumulated gradient\n    outputs = model(data_inputs)# forwardpass through model\n    loss = criterion(outputs,targets) # loss function\n    loss.backward() # compute the gradient for all weights\n    optimizer.step() # update the weights based on computed gradient\n    if (epoch+1)% 5 == 0:\n        print(f\"epoches[{epoch+1/epoches}],Loss:{loss.item():4f}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T17:42:19.162989Z","iopub.execute_input":"2025-07-06T17:42:19.163426Z","iopub.status.idle":"2025-07-06T17:42:19.240279Z","shell.execute_reply.started":"2025-07-06T17:42:19.163394Z","shell.execute_reply":"2025-07-06T17:42:19.239250Z"}},"outputs":[{"name":"stdout","text":"epoches[4.05],Loss:0.711552\nepoches[9.05],Loss:0.710071\nepoches[14.05],Loss:0.708405\nepoches[19.05],Loss:0.706537\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}